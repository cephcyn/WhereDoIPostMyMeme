{"version":3,"sources":["img/motivational-leo-v1.png","img/motivational-leo-v2.png","img/model_chart.png","img/stats-image.png","img/stats-language.png","img/example-awwnime.png","img/example-tumblr.png","img/example-dogelore.png","components/DropzoneIcon.js","components/DropImageCard.js","components/Scorecard.js","components/ModelDemo.js","components/App.js","reportWebVitals.js","index.js","components/utils.js"],"names":["makeStyles","icon","width","height","color","card","position","display","alignItems","justifyContent","marginBottom","canvas","zIndex","input","item","paddingTop","makeSession","theme","root","padding","demoElement","marginTop","submit","background","backgroundSize","border","borderRadius","boxShadow","shiny","animation","useStyles","align","panel","examplecard","left","transform","maxWidth","memetitletext","fontFamily","detailtext","shinybutton","shinypanel","App","classes","useState","motivationalLeo1","imgLeo","setImgLeo","Container","className","CssBaseline","Paper","style","textAlign","Typography","variant","gutterBottom","href","rel","ratio","src","frameBorder","allow","allowFullScreen","autoPlay","indicators","timeout","navButtonsAlwaysVisible","navButtonsAlwaysInvisible","exampleAwwnime","alt","aspectRatio","exampleTumblr","exampleDogelore","Button","statsLanguage","statsImage","modelchart","onClick","event","motivationalLeo2","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById","model_url","getLabelName","split","map","p","charAt","toUpperCase","slice","join","getImg","labelName","InferenceSession","backendHint","warmupModel","session","a","loadModel","wait","ms","Promise","res","rej","global","setTimeout","imgConfig","maxHeight","cover","crop","crossOrigin","orientation","getImage","url","loadImage","img","fetchImage","setData","current","type","Error","ctx","getContext","drawImage","data","getImageData","console","log"],"mappings":"oVAAe,G,OAAA,IAA0B,iDCA1B,MAA0B,gDCA1B,MAA0B,wCCA1B,MAA0B,wCCA1B,MAA0B,2CCA1B,MAA0B,4CCA1B,MAA0B,2CCA1B,MAA0B,6C,4CCKvBA,YAAW,CACzBC,KAAM,CACFC,MAAO,MACPC,OAAQ,MACRC,MAAO,U,aCHGJ,YAAW,CACzBK,KAAM,CACJH,MAAO,QACPC,OAAQ,QACRG,SAAU,WACVC,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBC,aAAc,IAEhBC,OAAQ,CACNT,MAAO,QACPC,OAAQ,QACRS,OAAQ,EACRN,SAAU,YAEZO,MAAO,CACLD,OAAQ,KACRN,SAAU,c,0CCbEN,YAAW,CACzBK,KAAM,CACFF,OAAQ,QAEZW,KAAM,CACFC,WAAY,M,YCHJC,cAEEhB,aAAW,SAACiB,GAAD,MAAY,CACvCC,KAAM,CACJC,QAAS,aAEXC,YAAa,CACXC,UAAW,OACXX,aAAc,OACdR,MAAO,QAEToB,OAAQ,CACNC,WAAY,2CACZC,eAAgB,YAChBC,OAAQ,EACRC,aAAc,EACdC,UAAW,wCACXvB,MAAO,QACPD,OAAQ,GACRgB,QAAS,UAEXS,MAAO,CAELL,WAAY,6DACZC,eAAgB,YAChBK,UAAW,8BACXF,UAAW,yCAEb,sBAAuB,CACtB,KAAM,CACL,sBAAuB,UAExB,MAAO,CACN,sBAAuB,YAExB,OAAQ,CACP,sBAAuB,eCxB3B,IAAMG,EAAY9B,aAAW,SAACiB,GAAD,MAAY,CACvCC,KAAM,CACJa,MAAO,UAETC,MAAO,CACLb,QAAS,YACTE,UAAW,OACXX,aAAc,QAEhBuB,YAAa,CACXd,QAAS,YACTb,SAAU,WACV4B,KAAM,MACNC,UAAW,qBACXC,SAAU,OAEZC,cAAe,CACbC,WAAY,kDAEdC,WAAY,CACVpB,QAAS,YACTE,UAAW,OACXX,aAAc,QAEhB8B,YAAa,CACXjB,WAAY,6DACZC,eAAgB,YAChBK,UAAW,8BACXJ,OAAQ,EACRC,aAAc,EACdC,UAAW,wCACXvB,MAAO,QACPD,OAAQ,GACRgB,QAAS,UAEXsB,WAAY,CACVlB,WAAY,6DACZC,eAAgB,YAChBK,UAAW,+BAEb,sBAAuB,CACtB,KAAM,CACL,sBAAuB,UAExB,MAAO,CACN,sBAAuB,YAExB,OAAQ,CACP,sBAAuB,eAKZ,SAASa,IACtB,IAAMC,EAAUb,IADY,EAGAc,mBAASC,GAHT,mBAGrBC,EAHqB,KAGbC,EAHa,KAY5B,OACE,eAACC,EAAA,EAAD,CAAWC,UAAWN,EAAQzB,KAA9B,UACE,cAACgC,EAAA,EAAD,IACA,eAACC,EAAA,EAAD,CAAOF,UAAS,UAAKN,EAAQX,MAAb,YAAsBW,EAAQF,YAAcW,MAAO,CAAEC,UAAU,UAA/E,UACE,cAACC,EAAA,EAAD,CAAYC,QAAQ,KAAKN,UAAWN,EAAQN,cAA5C,qBAGA,cAACiB,EAAA,EAAD,CAAYC,QAAQ,KAAKN,UAAWN,EAAQN,cAA5C,8DAIF,cAACc,EAAA,EAAD,CAAOF,UAAWN,EAAQX,MAA1B,SACE,eAACsB,EAAA,EAAD,glBASsB,uCATtB,igBAgBiD,4KAhBjD,SAqBF,eAACH,EAAA,EAAD,CAAOF,UAAWN,EAAQJ,WAA1B,UACE,cAACe,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,sBAGA,eAACF,EAAA,EAAD,mlCAiBG,mBAAGG,KAAK,kBAAkBC,IAAI,WAA9B,6BAjBH,wIAsBF,cAACP,EAAA,EAAD,CAAOF,UAAWN,EAAQX,MAAOoB,MAAO,CAAEC,UAAU,UAApD,SACE,cAAC,IAAD,CAAaM,MAAM,SAASP,MAAO,CAAEhB,SAAU,MAAOF,KAAM,MAAOC,UAAW,sBAA9E,SACE,wBACEyB,IAAI,qDACJC,YAAY,IACZC,MAAM,2FACNC,iBAAe,QAIrB,eAACZ,EAAA,EAAD,CAAOF,UAAWN,EAAQX,MAA1B,UACE,cAACsB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,sBAGA,cAACR,EAAA,EAAD,UACE,cAAC,IAAD,CACEgB,UAAU,EACVnC,UAAW,QACXoC,YAAY,EACZC,QAAS,IACTC,yBAAyB,EACzBC,2BAA2B,EAN7B,SASI,CACE,cAACjB,EAAA,EAAD,CAAOF,UAAWN,EAAQV,YAA1B,SACE,cAACe,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKS,EACLC,IAAI,8BACJC,YAAa,EAAE,QAIrB,cAACpB,EAAA,EAAD,CAAOF,UAAWN,EAAQV,YAA1B,SACE,cAACe,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKY,EACLF,IAAI,8BACJC,YAAa,EAAE,QAIrB,cAACpB,EAAA,EAAD,CAAOF,UAAWN,EAAQV,YAA1B,SACE,cAACe,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKa,EACLH,IAAI,8BACJC,YAAa,EAAE,gBAS/B,eAACpB,EAAA,EAAD,CAAOF,UAAS,UAAKN,EAAQX,MAAb,YAAsBW,EAAQF,YAA9C,UACE,cAACa,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,8BAWA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,UAA9B,SACE,cAACqB,EAAA,EAAD,CAAQzB,UAAWN,EAAQH,YAAaiB,KAAK,4EAA7C,+CAGJ,eAACN,EAAA,EAAD,CAAOF,UAAWN,EAAQX,MAA1B,UACE,cAACsB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,+BAGA,eAACL,EAAA,EAAD,CAAOF,UAAWN,EAAQJ,WAA1B,UACE,cAACe,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,0BAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,uyDAIF,eAACL,EAAA,EAAD,CAAOF,UAAWN,EAAQJ,WAA1B,UACE,cAACe,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,yBAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,o/BAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,usBAIF,eAACL,EAAA,EAAD,CAAOF,UAAWN,EAAQJ,WAA1B,UACE,cAACe,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,mCAGA,cAACF,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,mCAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,mMAGA,cAACF,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,oCAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,i+CAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,suCAGA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKe,EACLL,IAAI,8BACJC,YAAa,EAAE,MAGnB,cAACjB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,mCAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,moDAGA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKgB,EACLN,IAAI,2BACJC,YAAa,EAAE,MAGnB,cAACjB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,+BAGA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAUnD,MAAM,OAA9C,SACE,cAAC,IAAD,CACE0D,IAAKiB,EACLP,IAAI,sBAGR,cAAChB,EAAA,EAAD,CAAYE,cAAY,EAAxB,skDAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,skDAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,ikCAKJ,cAACL,EAAA,EAAD,CAAOF,UAAS,UAAKN,EAAQX,MAAb,YAAsBW,EAAQF,YAA9C,SACE,cAACO,EAAA,EAAD,CAAWI,MAAO,CAAElD,MAAM,OAA1B,SACE,cAAC,IAAD,CACE0D,IAAKd,EACLwB,IAAI,+CACJlE,MAAM,cACN0E,QApOQ,SAACC,GAEfhC,EADED,IAASD,EACDmC,EAEAnC,IAiOJO,MAAO,CAAEjD,OAAO,kBCvT5B,IAYe8E,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAClD,EAAD,MAEFmD,SAASC,eAAe,SAM1Bb,K,6QCVMc,G,YAAY,kFAGLC,EAAe,SAAA/C,GAAS,OAAIA,EAAUgD,MAAM,KAAKC,KAAI,SAAAC,GAChE,OAAOA,EAAEC,OAAO,GAAGC,cAAgBF,EAAEG,MAAM,MAC1CC,KAAK,MAEKC,EAAS,SAACC,GACrB,MAAO,uCAGIzF,EAEJ,WAIL,OAAO,IAAI0F,mBAAiB,CAC1BC,YAAa,W,SAKJC,E,8EAAf,WAA2BC,GAA3B,SAAAC,EAAA,2F,sBAWO,SAAeC,EAAtB,kC,4CAAO,WAAyBF,GAAzB,SAAAC,EAAA,sEAGCD,EAAQE,UAAUhB,GAHnB,uBAICa,EAAYC,GAJb,4C,sBAyBP,IA4BMG,EAAO,SAAAC,GAAE,OAAI,IAAIC,SAAQ,SAACC,EAAKC,GACnCC,EAAOC,YAAW,kBAAMH,MAAOF,OAG3BM,EAAY,CAChBnF,SAAU,IACVoF,UAAW,IACXC,OAAO,EACPC,MAAM,EACN/G,QAAQ,EACRgH,YAAa,YACbC,aAAa,GAGTC,EAAW,SAAAC,GAAG,OAAI,IAAIZ,SAAQ,SAACC,EAAKC,GACxCW,IAAUD,GAAK,SAAAE,GAAG,OAAIb,EAAIa,KAAMT,OAGrBU,EAAU,uCAAG,WAAOH,EAAKnH,EAAQuH,GAApB,mBAAApB,EAAA,yDACnBnG,GAAWA,EAAOwH,QADC,iEAENN,EAASC,GAFH,UAGP,WADXE,EAFkB,QAGhBI,KAHgB,sBAGQ,IAAIC,MAAM,wBAHlB,cAIlBC,EAAM3H,EAAOwH,QAAQI,WAAW,OAClCC,UAAUR,EAAK,EAAG,GALE,UAMlBhB,EAAK,GANa,QAOlByB,EAAOH,EAAII,aAAa,EAAG,EAAG/H,EAAOwH,QAAQjI,MAAOS,EAAOwH,QAAQhI,QACzEwI,QAAQC,IAAI,kBACZD,QAAQC,IAAIjI,EAAOwH,QAAQjI,MAAM,IAAIS,EAAOwH,QAAQhI,QACpDwI,QAAQC,IAAIH,GACZP,EAAQO,GAXgB,4CAAH,4D","file":"static/js/main.d16d324b.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/motivational-leo-v1.ef976b8d.png\";","export default __webpack_public_path__ + \"static/media/motivational-leo-v2.7c71dc7b.png\";","export default __webpack_public_path__ + \"static/media/model_chart.56d9d808.png\";","export default __webpack_public_path__ + \"static/media/stats-image.5285c4b1.png\";","export default __webpack_public_path__ + \"static/media/stats-language.e8c3434a.png\";","export default __webpack_public_path__ + \"static/media/example-awwnime.c9577563.png\";","export default __webpack_public_path__ + \"static/media/example-tumblr.88a44889.png\";","export default __webpack_public_path__ + \"static/media/example-dogelore.7de12bf1.png\";","import React from 'react';\nimport InsertPhotoIcon from '@material-ui/icons/InsertPhoto';\nimport CheckCircleIcon from '@material-ui/icons/CheckCircle';\nimport { makeStyles } from '@material-ui/core/styles';\n\nconst useStyles = makeStyles({\n    icon: {\n        width: '50%',\n        height: '50%',\n        color: 'grey',\n    },\n});\n\nexport default ({fileLoaded, isDragActive}) => {\n    const classes = useStyles();\n    if (fileLoaded) { return null; }\n    if (isDragActive) { return <CheckCircleIcon className={classes.icon} />; }\n    return <InsertPhotoIcon className={classes.icon} />;\n}\n","import React, {useCallback} from 'react';\nimport DropzoneIcon from './DropzoneIcon'\nimport {useDropzone} from 'react-dropzone';\nimport { makeStyles } from '@material-ui/core/styles';\nimport Card from '@material-ui/core/Card';\n\nconst useStyles = makeStyles({\n    card: {\n      width: '299px',\n      height: '299px',\n      position: 'relative',\n      display: 'flex',\n      alignItems: 'center',\n      justifyContent: 'center',\n      marginBottom: 10,\n    },\n    canvas: {\n      width: '299px',\n      height: '299px',\n      zIndex: 0,\n      position: 'absolute',\n    },\n    input: {\n      zIndex: 9999,\n      position: 'absolute',\n    },\n});\n\nexport default function DropImageCard({setFile, canvasRef, fileLoaded}) {\n  const classes = useStyles();\n  const onDrop = useCallback(acceptedFiles => {\n    if (acceptedFiles.length > 1) {\n      return console.log('Can only upload one file at a time');\n    }\n    if (acceptedFiles.length === 0) return;\n    const file = acceptedFiles[0];\n    if (!file.type.startsWith('image')) {\n      return console.log('File must be an image');\n    }\n    setFile(file);\n  }, [setFile])\n  const {getRootProps, getInputProps, isDragActive} = useDropzone({onDrop})\n\n  return (\n    <Card {...getRootProps()} className={classes.card}>\n      <canvas className={classes.canvas} ref={canvasRef} width={'299px'} height={'299px'} />\n      <input alt=\"Image Dropzone\" type=\"image\" className={classes.input} {...getInputProps()} />\n      <DropzoneIcon fileLoaded={fileLoaded} isDragActive={isDragActive} />\n    </Card>\n  )\n}\n","import React from 'react';\nimport Avatar from '@material-ui/core/Avatar';\nimport Card from '@material-ui/core/Card';\nimport Typography from '@material-ui/core/Typography';\nimport List from '@material-ui/core/List';\nimport ListItem from '@material-ui/core/ListItem';\nimport ListItemAvatar from '@material-ui/core/ListItemAvatar';\nimport ListItemText from '@material-ui/core/ListItemText';\nimport ListItemSecondaryAction from '@material-ui/core/ListItemSecondaryAction';\nimport { makeStyles } from '@material-ui/core/styles';\n\nconst useStyles = makeStyles({\n    card: {\n        height: 'auto',\n    },\n    item: {\n        paddingTop: 10,\n    },\n});\n\nexport default function Scorecard({items}) {\n    const classes = useStyles();\n    return <Card className={classes.card}><List dense>\n        {items.map(({avatar, name, percentage}) => {\n            const id = `${name}-${percentage}`\n            return <ListItem key={id} className={classes.item}>\n                <ListItemAvatar>\n                    <Avatar\n                        alt={`image of ${name}`}\n                        src={avatar}\n                    />\n                </ListItemAvatar>\n                <ListItemText id={id} primary={name} />\n                <ListItemSecondaryAction>\n                    <Typography>{percentage}%</Typography>\n                </ListItemSecondaryAction>\n            </ListItem>\n        })}\n    </List></Card>;\n};\n","import React, {useRef, useEffect, useState} from 'react';\nimport Container from '@material-ui/core/Container';\nimport CssBaseline from '@material-ui/core/CssBaseline';\nimport Typography from '@material-ui/core/Typography';\nimport Button from '@material-ui/core/Button';\nimport Grid from '@material-ui/core/Grid';\nimport TextField from '@material-ui/core/TextField';\nimport { makeStyles } from '@material-ui/core/styles';\n\nimport DropImageCard from './DropImageCard'\nimport Predictions from './Predictions'\nimport { fetchImage, makeSession, loadModel, runModel } from './utils'\n\nconst session = makeSession();\n\nconst useStyles = makeStyles((theme) => ({\n  root: {\n    padding: '15px 30px',\n  },\n  demoElement: {\n    marginTop: '10px',\n    marginBottom: '10px',\n    width: '100%',\n  },\n  submit: {\n    background: 'linear-gradient(45deg, #d08771, #c85b85)',\n    backgroundSize: '200% 200%',\n    border: 0,\n    borderRadius: 3,\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .1)',\n    color: 'white',\n    height: 48,\n    padding: '0 30px',\n  },\n  shiny: {\n    // I wonder if I can randomize the color lmao\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .5)',\n  },\n  '@keyframes gradient': {\n  \t'0%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  \t'50%': {\n  \t\t'background-position': '100% 50%',\n  \t},\n  \t'100%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  }\n}));\n\nexport default function ModelDemo() {\n  const [loaded, setLoaded] = useState(false);\n  const [isLoading, setIsLoading] = useState(false);\n  const startLoadModel = async () => {\n    if (isLoading || loaded) { return; }\n    setIsLoading(true);\n    await loadModel(session);\n    setLoaded(true);\n    setIsLoading(false);\n  }\n\n  const [file, setFile] = useState(null)\n  const canvas = useRef(null)\n  const [imgData, setImgData] = useState(null)\n  useEffect(() => {\n    console.log('file updated');\n    console.log(file);\n    if (file) fetchImage(file, canvas, setImgData);\n  }, [file])\n\n  const [textData, setTextData] = useState(\"\")\n  const handleTextChange = (event) => {\n    setTextData(event.target.value);\n  };\n\n  const [startedRun, setStartedRun] = useState(null);\n  const [outputMap, setOutputMap] = useState(null);\n  const startRunModel = async () => {\n    // if (!loaded || !imgData || !(textData.length>0)) return;\n    // setStartedRun(true);\n    console.log('clicked start button!');\n    console.log('image data: ')\n    console.log(imgData);\n    console.log('text data: '+textData);\n    // runModel(session, imgData, textData, setOutputMap);\n  };\n  useEffect(() => {\n    if (!loaded) return;\n    setStartedRun(false);\n  }, [outputMap, imgData, textData]); // runs when loaded or data changes\n  const outputData = outputMap && outputMap.values().next().value.data;\n\n  const classes = useStyles();\n  return (\n    <Container className={classes.root}>\n      <Grid container spacing={3}>\n        <Grid item xs={4}>\n          <Button className={`${classes.demoElement} ${classes.submit} ${classes.shiny}`} onClick={startRunModel}>TODO use CONSOLE, DELETE LATER</Button>\n          { !loaded && !isLoading && (<Button className={`${classes.demoElement} ${classes.submit}`} onClick={startLoadModel}>Load model (TODO 40 MB)</Button>) }\n          { !loaded && isLoading && (<Button className={`${classes.demoElement} ${classes.submit}`}>Loading model...</Button>) }\n          { loaded && !file && (<Button className={`${classes.demoElement} ${classes.submit}`}>Need to upload image</Button>) }\n          { loaded && file && !imgData && (<Button className={`${classes.demoElement} ${classes.submit}`}>Loading image...</Button>) }\n          { loaded && file && imgData && !(textData.length>0) && (<Button className={`${classes.demoElement} ${classes.submit}`}>Need to add text</Button>) }\n          { loaded && file && imgData && (textData.length>0) && !startedRun && (<Button className={`${classes.demoElement} ${classes.submit} ${classes.shiny}`} onClick={startRunModel}>WHERE SHOULD I POST THIS?</Button>) }\n          { loaded && startedRun && (<Button className={`${classes.demoElement} ${classes.submit}`}>Running model...</Button>) }\n          <Predictions output={outputData} className={classes.demoElement} />\n        </Grid>\n        <Grid item xs={8}>\n          <div style={{ display:'inline-block', position: 'relative', left: '50%', transform: 'translate(-50%, 0)' }}>\n            <DropImageCard setFile={setFile} canvasRef={canvas} fileLoaded={!!file} className={classes.demoElement} />\n          </div>\n          <TextField id=\"outlined-basic\" label=\"Meme Title\" variant=\"outlined\" value={textData} onChange={handleTextChange} className={classes.demoElement} />\n        </Grid>\n      </Grid>\n    </Container>\n  )\n}\n","import React, { useState } from 'react';\nimport Container from '@material-ui/core/Container';\nimport CssBaseline from '@material-ui/core/CssBaseline';\nimport Typography from '@material-ui/core/Typography';\nimport Paper from '@material-ui/core/Paper';\nimport Button from '@material-ui/core/Button';\nimport { makeStyles } from '@material-ui/core/styles';\n\nimport 'react-aspect-ratio/aspect-ratio.css'\nimport AspectRatio from 'react-aspect-ratio';\nimport Carousel from 'react-material-ui-carousel'\nimport Image from 'material-ui-image'\n\nimport \"fontsource-roboto\"\nimport motivationalLeo1 from './../img/motivational-leo-v1.png'\nimport motivationalLeo2 from './../img/motivational-leo-v2.png'\nimport modelchart from './../img/model_chart.png'\nimport statsImage from './../img/stats-image.png'\nimport statsLanguage from './../img/stats-language.png'\nimport exampleAwwnime from './../img/example-awwnime.png'\nimport exampleTumblr from './../img/example-tumblr.png'\nimport exampleDogelore from './../img/example-dogelore.png'\n\nimport ModelDemo from './ModelDemo'\n\nconst useStyles = makeStyles((theme) => ({\n  root: {\n    align: 'center',\n  },\n  panel: {\n    padding: '15px 30px',\n    marginTop: '20px',\n    marginBottom: '20px',\n  },\n  examplecard: {\n    padding: '10px 30px',\n    position: 'relative',\n    left: '50%',\n    transform: 'translate(-50%, 0)',\n    maxWidth: '80%',\n  },\n  memetitletext: {\n    fontFamily: 'Comic Sans MS, Comic Sans, Comic Neue, cursive',\n  },\n  detailtext: {\n    padding: '10px 30px',\n    marginTop: '15px',\n    marginBottom: '15px',\n  },\n  shinybutton: {\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n    border: 0,\n    borderRadius: 3,\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .5)',\n    color: 'white',\n    height: 48,\n    padding: '0 30px',\n  },\n  shinypanel: {\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n  },\n  '@keyframes gradient': {\n  \t'0%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  \t'50%': {\n  \t\t'background-position': '100% 50%',\n  \t},\n  \t'100%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  }\n}));\n\nexport default function App() {\n  const classes = useStyles();\n\n  const [imgLeo, setImgLeo] = useState(motivationalLeo1)\n  const toggleLeo = (event) => {\n    if (imgLeo===motivationalLeo1) {\n      setImgLeo(motivationalLeo2);\n    } else {\n      setImgLeo(motivationalLeo1);\n    }\n  };\n\n  return (\n    <Container className={classes.root}>\n      <CssBaseline />\n      <Paper className={`${classes.panel} ${classes.shinypanel}`} style={{ textAlign:'center' }}>\n        <Typography variant=\"h1\" className={classes.memetitletext}>\n          MemeNet\n        </Typography>\n        <Typography variant=\"h4\" className={classes.memetitletext}>\n          Multimodal Models Make Meme Market Manageable\n        </Typography>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography>\n          Artificial Intelligence (A.I.) has been applied in areas such as\n          economics and algorithmic trading to great effect. In recent decades,\n          the rise of viral Internet culture has led to the development of a new\n          global economy: the online \"meme economy\". Drawing from scarce resources\n          (such as creativity, humor, and time), individual producers (meme\n          makers) offer their goods (memes in the form of multimodal ideas) over a\n          centralized marketplace (Internet forums such as subreddits on Reddit)\n          in exchange for currency (Internet points such as Upvotes or Likes).\n          Oftentimes, knowing <em>where</em> to post a meme can greatly affect how\n          well it is received by the Internet community. Posting in a highly apt\n          channel can lead to instant Internet fame, while posting in a suboptimal\n          channel can lead to one's creative work failing to gain attention, or\n          worse, being stolen and reposted by meme thieves. Additionally, posting\n          the same content in several different channels can be considered\n          &quot;spamming&quot; and is negatively regarded. To make this decision easier for\n          the millions of meme creators on the Internet, <strong>we developed a\n          multimodal neural network to predict the single best subreddit that a\n          given meme should be posted to for maximum profit</strong>.\n        </Typography>\n      </Paper>\n      <Paper className={classes.detailtext}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Abstract\n        </Typography>\n        <Typography>\n          Deep neural networks are excellent at learning from data that consists\n          of single modalities. For example, convolutional neural networks are\n          highly performant on image classification, and sequence models are the\n          state-of-the-art for text generation. However, media such as Internet\n          memes often consist of multiple modalities. A meme may have an image\n          component and a text component, each of which contribute information\n          about what the meme is trying to convey. To extract features from\n          multimodal data, we leverage multimodal deep learning, in which we use\n          multiple feature extractor networks to learn the separate modes\n          individually, and an aggregator network to combine the features to\n          produce the final output classification. We scrape Reddit meme\n          subreddits for post data, including: subreddit name, upvote/downvote\n          count, images, meme text via OCR (or human OCR), and post titles. We\n          construct a train and test set and evaluate results using a\n          precision/accuracy measure for subreddit name predictions. To optimize\n          our model, we use FAIR’s open source multimodal library, Pythia/MMF\n          (<a href=\"https://mmf.sh/\" rel=\"nofollow\">https://mmf.sh/</a>), and try\n          a variety of model architectures and hyperparameters. Finally, we include\n          our best model for demonstration purposes.\n        </Typography>\n      </Paper>\n      <Paper className={classes.panel} style={{ textAlign:'center' }}>\n        <AspectRatio ratio=\"16 / 9\" style={{ maxWidth: '60%', left: '50%', transform: 'translate(-50%, 0)' }}>\n          <iframe\n            src=\"https://www.youtube-nocookie.com/embed/LpU8CUmxcI8\"\n            frameBorder=\"0\"\n            allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n            allowFullScreen>\n          </iframe>\n        </AspectRatio>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Examples\n        </Typography>\n        <Container>\n          <Carousel\n            autoPlay={false}\n            animation={\"slide\"}\n            indicators={true}\n            timeout={500}\n            navButtonsAlwaysVisible={true}\n            navButtonsAlwaysInvisible={false}\n          >\n            {\n              [\n                <Paper className={classes.examplecard}>\n                  <Container style={{ textAlign:'center', width:'50%' }}>\n                    <Image\n                      src={exampleAwwnime}\n                      alt=\"Results for language models\"\n                      aspectRatio={5/3}\n                    />\n                  </Container>\n                </Paper>,\n                <Paper className={classes.examplecard}>\n                  <Container style={{ textAlign:'center', width:'50%' }}>\n                    <Image\n                      src={exampleTumblr}\n                      alt=\"Results for language models\"\n                      aspectRatio={5/3}\n                    />\n                  </Container>\n                </Paper>,\n                <Paper className={classes.examplecard}>\n                  <Container style={{ textAlign:'center', width:'50%' }}>\n                    <Image\n                      src={exampleDogelore}\n                      alt=\"Results for language models\"\n                      aspectRatio={5/3}\n                    />\n                  </Container>\n                </Paper>\n              ]\n            }\n          </Carousel>\n        </Container>\n      </Paper>\n      <Paper className={`${classes.panel} ${classes.shinypanel}`}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Try It Yourself!\n        </Typography>\n        {/* The built-in demo is not currently working, so I'm just linking to public Colab file. */}\n        {\n        // <Paper style={{ background: '#EBEBEB' }}>\n        //   <Container>\n        //     <ModelDemo />\n        //   </Container>\n        // </Paper>\n        }\n        <Container style={{ textAlign:'center' }}>\n          <Button className={classes.shinybutton} href=\"https://colab.research.google.com/drive/1139WDXzKaWsXPr2rUKzH5Vt8C8ZnFA9k\">Check it out on Google Colab</Button>\n        </Container>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Behind The Scenes\n        </Typography>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Related Work\n          </Typography>\n          <Typography gutterBottom>\n            Our project was inspired by Facebook AI’s “Hateful Memes Challenge” in which participants developed novel model architectures to detect harmful multimodal content. The Facebook meme dataset consists of ~10,000 multimodal examples. Based on our personal understanding of the Internet meme culture, we decided that a larger dataset needed to be collected in order to reasonably represent the various subcultures on the Internet. Accordingly, we scraped meme data from Reddit, a popular hub for sharing meme content, which totals 70,000+ examples. Additionally, instead of a binary classification problem, we defined a multi-class classification problem in which our model has to output the most apt Reddit subreddit that a given meme fits. The rationale behind this decision was that different communities on the Internet operate by different de facto rules and guidelines. It is difficult to prescribe a blanket hateful/un-hateful categorization for all memes shared on the Internet. From a human perspective, a meme is usually considered within a given context. For example, particularly dark or edgy jokes may be perfectly acceptable in a community such as r/dankmemes but unacceptable according to Facebook’s platform guidelines. Hence, our work on multimodal multi-class classification could serve as a first step into exploring the effect that the style of multimodal content has on whether or not it is considered hateful. Furthermore, our model can be used by meme creators in deciding where to best post their meme. Optimizing this portion of the meme economy could be highly impactful in facilitating a less hateful Internet community, because by determining the most appropriate channels for memes, creators can avoid posting their work to places where it would be negatively received.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Methodology\n          </Typography>\n          <Typography gutterBottom>\n            The final goal of this project was to build a multimodal model to predict which subreddit a meme was posted in, using both the image and title text components. First, we scraped around 80,000 posts worth of meme data from Reddit (including important metadata such as post title, number of upvotes, etc) and labeled each meme with the subreddit it was sourced from. We included 19 different subreddits, which represents a good variety of multimodal content that people enjoy sharing on the Internet. Importantly, while some subreddits are stylistically very distinct from each other yet feature similar post title conventions (dankmemes vs. tumblr), others are more difficult to distinguish from each other just by looking at the image, even for humans (meirl vs. 2meirl4meirl). Hence, if our model is able to achieve high predictive performance, we can be certain to a large extent that our methodology is appropriately enabling the model to extract information from both text and image modalities.\n          </Typography>\n          <Typography gutterBottom>\n            Next, we created some baseline models that classify a meme based on text only, as well as models based on image only. The results of these models were compared to a model that classified based on both image and text, with the expectation that the multimodal model should perform better because it has more information it can use to predict. Additionally, we utilized transfer learning by incorporating BERT into the language portion of our architecture, and using a network pre-trained on ImageNet for the image portion of our architecture. This decision allowed us to leverage well-engineered basic features and focus our development efforts on learning features that are more unique to memes.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Experiments & Results\n          </Typography>\n          <Typography variant=\"h6\" style={{ textAlign:'center' }} gutterBottom>\n            Human Branch (Humans)\n          </Typography>\n          <Typography gutterBottom>\n            Before building any models, we established human-level performance by having our team of five label 91 randomly sampled memes from our dataset and calculating our combined accuracy.\n          </Typography>\n          <Typography variant=\"h6\" style={{ textAlign:'center' }} gutterBottom>\n            Language Branch (BERT)\n          </Typography>\n          <Typography gutterBottom>\n            The next experiment we performed was with the baseline language model. This model used word level representations of the title, instead of character level representations because we thought that key words in the title would be important for predicting what subreddit a meme would be posted in. For this approach, we scanned the 7000 titles of the posts, and for each word that appeared more than 30 times, it was assigned a number. There were 2420 words which were considered part of our vocab in the post titles, so each title was converted to a 1 by 2420 vector, where the index “i” in the vector was set to 1 if the word assigned the number “i” appeared in the title. This vector was used as an input to a linear network, with the first layer having 2420 input neurons and 512 output neurons, while the second layer had 512 input neurons and 20 output neurons because there were 20 output classes to predict. The activation functions used were leaky relu, and the network was trained for 40 epochs with a learning rate of 0.01. Using a subset of the data with 8000 train samples and 2000 test samples, which only had 16 classes (the second linear layer was changed to have 16 output neurons), the model was able to get 51% accuracy on the test set. When using the full dataset with around 70000 examples total, and 20 output classes, the model reached around 33% accuracy after training for 40 epochs, and the accuracy did not appear to increase with further training.\n          </Typography>\n          <Typography gutterBottom>\n            Because the full data set of 70,000 post titles (50,000 train) likely doesn’t capture enough language to generalize well, we then decided to try a pretrained model, namely BERT. This was done via the pretrained BERT library in PyTorch. Using the built-in tokenizer in the package, we separated each post title to 20 words (either truncating if there were more or padding with “[PAD]”) then used this as input into the model, which output a 1x20x768 tensor for 1 sample. Besides this we chose not to do any other significant preprocessing to the post titles that would normally be done in other language models, like lemmatizing or removing special characters as the post titles may rely on these features for meaning (eg emojis and utf-8 characters for Subs like r/surrealmemes). We then fed this output into a few fully-connected layers with ReLU activation before the prediction layer. However, due to computational limitations we added a convolutional layer after the output of BERT to downsize the number of weights needed in the fully-connected layers. Splitting the full data set into 80% train and 20% validation, after training for 4 epochs the accuracy was hovering around 0.62869 on the validation set (9028/14360).\n          </Typography>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n            <Image\n              src={statsLanguage}\n              alt=\"Results for language models\"\n              aspectRatio={8/3}\n            />\n          </Container>\n          <Typography variant=\"h6\" style={{ textAlign:'center' }} gutterBottom>\n            Image Branch (VGG-11)\n          </Typography>\n          <Typography gutterBottom>\n            The second experiment we performed focused on the baseline image model. Instead of initializing a convolutional neural network with random weights, we experimented with a variety of pretrained architectures that are well known for their high predictive performance (results shown in Figure 2 below). This design decision made it much easier to load in the dataset, since many of our examples are very large (on the order of 100M pixels) and transfer learning requires less data. However, we still sample from the full dataset. Specifically, the data was split into a training set and a validation set, with 200 examples in the training set for each category, and 100 examples in the validation set for each category. We chose to fine-tune the convnet on the meme data instead of freezing the pretrained weights as a fixed feature extractor. This decision was supported by our exploratory experimentation in which we found that a pretrained ResNet-18 model, when fine-tuned on meme images, achieved 0.015 higher accuracy when compared to when the weights were frozen. This finding makes sense because meme formats are typically not represented in the ImageNet dataset, so some domain shift is necessary. Also informed by our exploratory experimentation, we apply data augmentation in the form of random crops and horizontal flips. The most impactful hyperparameters we found were learning rate and momentum, which we set to 0.001 and 0.9, respectively. Finally, to aid convergence, we decay the learning rate by a factor of 0.1 every 7 epochs. The results of this experiment guided how we developed the image component of our final model, detailed below.\n          </Typography>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n            <Image\n              src={statsImage}\n              alt=\"Results for image models\"\n              aspectRatio={7/3}\n            />\n          </Container>\n          <Typography variant=\"h6\" style={{ textAlign:'center' }} gutterBottom>\n            Multimodal Branch\n          </Typography>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n            <Image\n              src={modelchart}\n              alt=\"Model structure\"\n            />\n          </Container>\n          <Typography gutterBottom>\n            Our final round of experimentation focused on a combined multimodal model. We’ve included a figure of this model’s architecture above. Our model has two modalities: a text modality that takes in a post’s string title, and an image modality that takes in the meme you want to post, formatted as an RGB 3x256x256 tensor. The model has two branches, one for each modality, and obtains derived representations of each modality that it then combines into a single tensor. For the text modality, we simply run the title through a pretrained BERT model. We cap title length at 20 words, and truncate titles longer and pad sentences that are shorter. For the image modality, we had a six layer convolutional network with ReLU activation layers between the convolutional layers, batch norm layers every second layer, dropout layers after the fourth and sixth convolutional layers, and a maxpooling layer after the sixth convolutional layer. After obtaining the outputs of each branch, i.e. the output of the BERT model and the output of the CNN, we fused both modalities together by flattening the outputs and concatenating them, which the text modality’s output coming before the image modality’s output. Then, our model finished with a five layer linearly connected network that outputs a 1 dimensional 19 element tensor where the index of the largest element in the tensor corresponds to a subreddit, which the model predicts the meme and title to be posted in. We ran this multimodal model for 10 epochs over the training data, and achieved a final test accuracy of 91.8%.\n          </Typography>\n          <Typography gutterBottom>\n            Our final round of experimentation focused on a combined multimodal model. We’ve included a figure of this model’s architecture above. Our model has two modalities: a text modality that takes in a post’s string title, and an image modality that takes in the meme you want to post, formatted as an RGB 3x256x256 tensor. The model has two branches, one for each modality, and obtains derived representations of each modality that it then combines into a single tensor. For the text modality, we simply run the title through a pretrained BERT model. We cap title length at 20 words, and truncate titles longer and pad sentences that are shorter. For the image modality, we had a six layer convolutional network with ReLU activation layers between the convolutional layers, batch norm layers every second layer, dropout layers after the fourth and sixth convolutional layers, and a maxpooling layer after the sixth convolutional layer. After obtaining the outputs of each branch, i.e. the output of the BERT model and the output of the CNN, we fused both modalities together by flattening the outputs and concatenating them, which the text modality’s output coming before the image modality’s output. Then, our model finished with a five layer linearly connected network that outputs a 1 dimensional 19 element tensor where the index of the largest element in the tensor corresponds to a subreddit, which the model predicts the meme and title to be posted in. We ran this multimodal model for 10 epochs over the training data, and achieved a final test accuracy of 91.8%.\n          </Typography>\n          <Typography gutterBottom>\n            Overall, our model did quite well. The multimodal modal model far outstripped the best text only and image only models, which had performances 63.8% and 68.1% respectively. One thing we observed was that for predicting some subs, the image tended to be a pretty big clue, like greentext or deepfriedmemes, while for others, the title was the big giveaway, like me_irl. The multimodal models were able to leverage both modalities, while the unimodal models couldn’t. Additionally, there are other more general subs where it was difficult to tell which sub they belonged to based on the meme or the title alone, like dankmemes. For these subs, the multimodal model was able to detect complex patterns and associations between the title and the image that we humans could not. In fact, humans did worse than all the models, with an accuracy of 62.6%. Humans especially struggled on the more general subreddits, where they were often able to narrow it down to 2 or 3 candidate subreddits, but could not correctly figure out which one of them was the correct subreddit.\n          </Typography>\n        </Paper>\n      </Paper>\n      <Paper className={`${classes.panel} ${classes.shinypanel}`}>\n        <Container style={{ width:\"40%\" }}>\n          <Image\n            src={imgLeo}\n            alt=\"Leo DiCaprio numpy meme (credits: Will Chen)\"\n            color=\"transparent\"\n            onClick={toggleLeo}\n            style={{ height:\"100px\" }}\n          />\n        </Container>\n      </Paper>\n    </Container>\n  );\n}\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './components/App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n","import loadImage from 'blueimp-load-image';\nimport { Tensor, InferenceSession } from 'onnxjs';\nimport ndarray from 'ndarray';\nimport ops from 'ndarray-ops';\n// import model from '../dogs-resnet18.onnx';\n\nconst model_url = 'https://github.com/davidpfahler/react-ml-app/raw/master/src/dogs-resnet18.onnx'\n// const model_url = '../dogs-resnet18.onnx'\n\nexport const getLabelName = className => className.split('_').map(p => {\n  return p.charAt(0).toUpperCase() + p.slice(1)\n}).join(' ')\n\nexport const getImg = (labelName) => {\n  return 'https://i.redd.it/vb4uq6nipk251.jpg'\n}\n\nexport const makeSession = (() => {\n  let _session = null;\n  return () => {\n    if (_session !== null) {\n      return _session;\n    }\n    return new InferenceSession({\n      backendHint: 'webgl'\n    });\n  }\n})()\n\nasync function warmupModel(session) {\n  // TODO does this even make sense to run for our model?\n  // const dims = [1, 3, 299, 299];\n  // const size = dims.reduce((a, b) => a * b);\n  // const warmupTensor = new Tensor(new Float32Array(size), 'float32', dims);\n  // for (let i = 0; i < size; i++) {\n  //     warmupTensor.data[i] = Math.random() * 2.0 - 1.0; // random value [-1.0, 1.0)\n  // }\n  // await session.run([warmupTensor]);\n}\n\nexport async function loadModel(session) {\n  // TODO swap out on deploying?\n  // await session.loadModel(model);\n  await session.loadModel(model_url);\n  await warmupModel(session);\n}\n\nasync function _runModel(session, imgInput, textInput, setOutputMap) {\n  const {\n    width,\n    height\n  } = imgInput;\n  // TODO modify\n  const data = preprocess(imgInput);\n  const imgInputTensor = new Tensor(data, 'float32', [1, 3, width, height]);\n  // await wait(0);\n  const outputMap = await session.run([imgInputTensor]);\n  setOutputMap(outputMap);\n}\n\nexport function runModel(session, imgInput, textInput, setOutputMap) {\n  setTimeout(() => _runModel(session, imgInput, textInput, setOutputMap), 10);\n}\n\n// borrowed from onnx.js example: https://github.com/microsoft/onnxjs/blob/4085b7e61804d093e36af6a456d8c14c329f0a0a/examples/browser/resnet50/index.js#L29\nconst preprocess = input => {\n  // rescale images to 3x256x256 TODO...\n  console.log(input)\n  const {\n    data,\n    width,\n    height\n  } = input\n\n  // data processing\n  const dataTensor = ndarray(new Float32Array(data), [width, height, 4]);\n  const dataProcessedTensor = ndarray(new Float32Array(width * height * 3), [1, 3, width, height]);\n  ops.assign(dataProcessedTensor.pick(0, 0, null, null), dataTensor.pick(null, null, 0));\n  ops.assign(dataProcessedTensor.pick(0, 1, null, null), dataTensor.pick(null, null, 1));\n  ops.assign(dataProcessedTensor.pick(0, 2, null, null), dataTensor.pick(null, null, 2));\n  ops.divseq(dataProcessedTensor, 255);\n  ops.subseq(dataProcessedTensor.pick(0, 0, null, null), 0.485);\n  ops.subseq(dataProcessedTensor.pick(0, 1, null, null), 0.456);\n  ops.subseq(dataProcessedTensor.pick(0, 2, null, null), 0.406);\n  ops.divseq(dataProcessedTensor.pick(0, 0, null, null), 0.229);\n  ops.divseq(dataProcessedTensor.pick(0, 1, null, null), 0.224);\n  ops.divseq(dataProcessedTensor.pick(0, 2, null, null), 0.225);\n\n  console.log(dataProcessedTensor);\n\n  return dataProcessedTensor.data;\n}\n\nconst wait = ms => new Promise((res, rej) => {\n  global.setTimeout(() => res(), ms)\n});\n\nconst imgConfig = {\n  maxWidth: 299,\n  maxHeight: 299,\n  cover: true,\n  crop: true,\n  canvas: true,\n  crossOrigin: 'Anonymous',\n  orientation: true,\n};\n\nconst getImage = url => new Promise((res, rej) => {\n  loadImage(url, img => res(img), imgConfig)\n});\n\nexport const fetchImage = async (url, canvas, setData) => {\n  if (!canvas || !canvas.current) return;\n  const img = await getImage(url);\n  if (img.type === \"error\") throw new Error(\"could not load image\");\n  const ctx = canvas.current.getContext('2d');\n  ctx.drawImage(img, 0, 0);\n  await wait(1);\n  const data = ctx.getImageData(0, 0, canvas.current.width, canvas.current.height);\n  console.log('in fetchImage,');\n  console.log(canvas.current.width+' '+canvas.current.height)\n  console.log(data)\n  setData(data);\n};\n"],"sourceRoot":""}