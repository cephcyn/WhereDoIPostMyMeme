{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of subreddits to collect posts from\n",
    "sub_list = ['greentext', 'deepfriedmemes', 'me_irl', '2meirl4meirl', 'dankmemes', \n",
    "            'adviceanimals', 'dogelore', 'wholesomememes', 'CatsPlayingDnd', 'glitch_art', \n",
    "            'awwnime', 'EarthPorn', 'tumblr', 'NapkinMemes', 'chickenswearingpants', \n",
    "            'BreadStapledToTrees', 'terriblefacebookmemes', 'Animemes', 'dataisbeautiful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW methodology\n",
    "# import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW methodology\n",
    "# reddit = praw.Reddit(\n",
    "#     site_name='scraperbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW methodology\n",
    "# def scrape_sub(count_each, scraper_funcs=[]):\n",
    "#     data = {}\n",
    "#     for func in scraper_funcs:\n",
    "#         for post in func(limit=count_each):\n",
    "#             if not post.over_18:\n",
    "#                 if post.url[-4:] == '.jpg':\n",
    "#                     data[post.id] = (post.title, post.url, post.subreddit.display_name, post.created_utc, post.score, post.upvote_ratio)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW methodology\n",
    "# data = {}\n",
    "\n",
    "# for sub_name in sub_list:\n",
    "#     subreddit = reddit.subreddit(sub_name)\n",
    "#     data.update(scrape_sub(1000, scraper_funcs=[subreddit.hot, subreddit.new, subreddit.rising, subreddit.top]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "pickle.HIGHEST_PROTOCOL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts(post_df):\n",
    "    output_df = post_df\n",
    "    output_df = output_df[output_df.is_self == False]\n",
    "    output_df = output_df[output_df.over_18 == False]\n",
    "    output_df = output_df[output_df.pinned == False]\n",
    "    output_df = output_df[output_df.score >= 5]\n",
    "    output_df = output_df[output_df.url.str.endswith('.jpg', na=False)]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect approximately `min_posts` number of posts (or the total # of posts in subreddit, whichever is smaller)\n",
    "# Collect posts from `subreddit`\n",
    "# Output the data file to `output_filename`\n",
    "def scrape_subreddit(min_posts, subreddit):\n",
    "    # get the sub creation date\n",
    "    with urllib.request.urlopen(f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size=1&sort=asc&before=30d\") as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    df_sub = pd.DataFrame.from_dict(pd.json_normalize(data['data']), orient='columns')\n",
    "    created_utc_first_sub = df_sub.tail(1)['created_utc']\n",
    "\n",
    "    # get the most recent N posts\n",
    "    with urllib.request.urlopen(f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size=1&sort=desc&before=30d\") as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    df_sub = pd.DataFrame.from_dict(pd.json_normalize(data['data']), orient='columns')\n",
    "    created_utc_now_sub = df_sub.tail(1)['created_utc']\n",
    "    \n",
    "    # make sure we get at least args.post_count number of posts scraped\n",
    "    # but also don't go past the beginning of the subreddit\n",
    "    try:\n",
    "        while (len(df_sub.index) < min_posts) and (int(created_utc_now_sub) > int(created_utc_first_sub)): \n",
    "            with urllib.request.urlopen(f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size=1000&sort=desc&before=%d\"%created_utc_now_sub) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "            df_new_sub = pd.DataFrame.from_dict(pd.json_normalize(data['data']), orient='columns')\n",
    "            df_sub = df_sub.append(df_new_sub)\n",
    "            created_utc_now_sub = df_sub.tail(1)['created_utc']\n",
    "            df_sub = filter_posts(df_sub)\n",
    "            print(len(df_sub))\n",
    "    finally:\n",
    "        # Clean up the post data that we scraped\n",
    "        df_sub = filter_posts(df_sub)\n",
    "        df_sub = df_sub.set_index('id')\n",
    "        df_sub = df_sub[~df_sub.index.duplicated(keep='first')]\n",
    "        df_sub = df_sub[['author', 'author_flair_text', 'created_utc', 'permalink', 'retrieved_on', \n",
    "                         'score', 'subreddit', 'title', 'upvote_ratio', 'url']]\n",
    "        return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit = 'deepfriedmemes'\n",
    "# with urllib.request.urlopen(f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size=100&sort=desc&before=30d\") as url:\n",
    "#     data = json.loads(url.read().decode())\n",
    "# df_sub = pd.DataFrame.from_dict(pd.json_normalize(data['data']), orient='columns')\n",
    "# df_sub[['over_18', 'score', 'title', 'url', 'permalink']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame()\n",
    "\n",
    "for sub_name in ['me_irl']:#sub_list:\n",
    "    scraped_df = scrape_subreddit(101, sub_name)\n",
    "    print('final grab', len(scraped_df))\n",
    "    dataset_df = pd.concat([dataset_df, scraped_df])\n",
    "    dataset_df = dataset_df.append(scraped_df)\n",
    "# dataset_df.to_hdf('data/dataset.h5', key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf('data/dataset.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('data/dataset.h5', 'df')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.subreddit.str.lower() == 'DeepFriedMemes'.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.duplicated(keep='first')]\n",
    "df = df[['author', 'author_flair_text', 'created_utc', 'permalink', 'retrieved_on',\n",
    "         'score', 'subreddit', 'title', 'upvote_ratio', 'url']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data/dataset.h5'):\n",
    "    os.remove('data/dataset.h5')\n",
    "df.to_hdf('data/dataset.h5', key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
