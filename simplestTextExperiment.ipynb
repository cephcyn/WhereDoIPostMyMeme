{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" badRNN.ipynb","provenance":[],"authorship_tag":"ABX9TyOC+wIXvUgC9edbKM+j5yrF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmm--PxpzaWd","executionInfo":{"status":"ok","timestamp":1608344934681,"user_tz":480,"elapsed":17757,"user":{"displayName":"Anthony Lu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTUIfTM4nze0CZKfQ2Xt4yh29AxmcfpTApeKWYiw=s64","userId":"06896095403680468971"}},"outputId":"3e44c6ad-7b2a-4dd6-de11-0ae5b06aee82"},"source":["import nltk\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer \r\n","import string\r\n","from nltk.tokenize import word_tokenize\r\n","import re\r\n","\r\n","stop = stopwords.words('english')\r\n","\r\n","lemmatizer = WordNetLemmatizer()\r\n","\r\n","#lowercase, remove stop words, strip punctuation\r\n","def clean_text(text):\r\n","    text = text.translate(str.maketrans('', '', string.punctuation))\r\n","    text = text.lower().strip()\r\n","    text = ' '.join([i if i not in stop and i.isalpha() else '' for i in text.lower().split()])\r\n","    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\r\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\r\n","    return text\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","!ls \"/content/drive/My Drive/Colab Notebooks\"\r\n","!cp \"/content/drive/My Drive/Colab Notebooks/subreddits.txt\" \"subeddits.txt\"\r\n","\r\n","post_titles_file = open('drive/My Drive/Colab Notebooks/post_titles.txt', 'r', encoding='utf-8')\r\n","post_titles = post_titles_file.readlines()\r\n","\r\n","number_samples_to_use = len(post_titles)\r\n","\r\n","counter = 1\r\n","vocab = {'unknown': 0}\r\n","vocab_to_occurrences = {}\r\n","#for i in range(len(post_titles)):\r\n","for i in range(number_samples_to_use):\r\n","    post_titles[i] = clean_text(post_titles[i])\r\n","    words_in_title = post_titles[i].split()\r\n","    for word in words_in_title:\r\n","        if word not in vocab_to_occurrences.keys():\r\n","            vocab_to_occurrences[word] = 1\r\n","        else:\r\n","            vocab_to_occurrences[word] = vocab_to_occurrences[word] + 1\r\n","#for i in range(len(post_titles)):\r\n","for i in range(number_samples_to_use):\r\n","    post_titles[i] = clean_text(post_titles[i])\r\n","    words_in_title = post_titles[i].split()\r\n","    for word in words_in_title:\r\n","        if word not in vocab_to_occurrences.keys():\r\n","            vocab_to_occurrences[word] = 1\r\n","        else:\r\n","            vocab_to_occurrences[word] = vocab_to_occurrences[word] + 1\r\n","        if word not in vocab.keys():\r\n","            if vocab_to_occurrences[word] >= 30:\r\n","                vocab[word] = counter\r\n","                counter += 1\r\n","\r\n","#might want to preprocess so that words which hardly appear do not come up, currently 28797 words\r\n","print(len(vocab))\r\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","' badRNN.ipynb'        post_titles.txt\t Untitled\n"," hw1.ipynb\t       subreddits.txt\t Untitled0.ipynb\n"," mnist_pytorch.ipynb   TextOnly.ipynb\t Untitled1.ipynb\n","2420\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ovaIn2phYtf_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608351948820,"user_tz":480,"elapsed":7798,"user":{"displayName":"Anthony Lu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTUIfTM4nze0CZKfQ2Xt4yh29AxmcfpTApeKWYiw=s64","userId":"06896095403680468971"}},"outputId":"ce6833d2-f99a-4580-82eb-f974929da713"},"source":["import torch\r\n","import random\r\n","\r\n","subeddits_file = open('drive/My Drive/Colab Notebooks/subreddits.txt', 'r', encoding='utf-8')\r\n","subreddits = subeddits_file.readlines()\r\n","subreddit_to_number = {}\r\n","number_to_subreddit = {}\r\n","\r\n","vocab_size = len(vocab)\r\n","\r\n","subr_counter = 0\r\n","#for i in range(len(subreddits)):\r\n","for i in range(number_samples_to_use):\r\n","    if subreddits[i] not in subreddit_to_number.keys():\r\n","        subreddit_to_number[subreddits[i]] = subr_counter\r\n","        number_to_subreddit[str(subr_counter)] = subreddits[i]\r\n","        subr_counter += 1\r\n","num_subreddits = len(subreddit_to_number)\r\n","print(num_subreddits)\r\n","\r\n","\r\n","formatted_data = []\r\n","#for i in range(len(subreddits)):\r\n","for i in range(number_samples_to_use):\r\n","    cur_title = post_titles[i]\r\n","    cur_subreddit = subreddits[i]\r\n","    datapoint = torch.zeros((vocab_size, 1))\r\n","    for word in cur_title:\r\n","        if word in vocab.keys():\r\n","            datapoint[vocab[word]] = 1\r\n","    datapoint = datapoint.transpose(0, 1)\r\n","    label = subreddit_to_number[cur_subreddit]\r\n","    label = torch.LongTensor([label])\r\n","    tuple_data = (datapoint, label)\r\n","    formatted_data.append(tuple_data)\r\n","\r\n","#randomly rearrange\r\n","#for i in range(2*len(formatted_data)):\r\n","#    first_index = random.randint(0, len(formatted_data ) - 1)\r\n","#    second_index = random.randint(0, len(formatted_data) - 1)\r\n","\r\n","#    temp = formatted_data[first_index]\r\n","#    formatted_data[first_index] = formatted_data[second_index]\r\n","#    formatted_data[second_index] = temp\r\n","\r\n","#    temp = subreddits[first_index]\r\n","#    subreddits[first_index] = subreddits[second_index]\r\n","#    subreddits[second_index] = temp\r\n","\r\n","num_subset = 20000\r\n","formatted_data = formatted_data[:num_subset]"],"execution_count":48,"outputs":[{"output_type":"stream","text":["20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N-TMsSjl7NuU","executionInfo":{"status":"ok","timestamp":1608344975183,"user_tz":480,"elapsed":353,"user":{"displayName":"Anthony Lu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTUIfTM4nze0CZKfQ2Xt4yh29AxmcfpTApeKWYiw=s64","userId":"06896095403680468971"}}},"source":["import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","class FastWordModel(nn.Module):\r\n","    def __init__(self, vocab_size, num_classes):\r\n","        super().__init__()\r\n","        self.lin1 = nn.Linear(vocab_size, 512)\r\n","        self.lin2 = nn.Linear(512, num_classes)\r\n","        #1 by vocab size -> 1 by 512 -> 1 by num_classes\r\n","\r\n","    def forward(self, x):\r\n","        x = F.leaky_relu(self.lin1(x))\r\n","        return F.leaky_relu(self.lin2(x))\r\n","    \r\n","    def loss(self, prediction, label):\r\n","        loss_val = F.cross_entropy(prediction, label)\r\n","        return loss_val"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPHsrrWf87WS","executionInfo":{"status":"ok","timestamp":1608353258647,"user_tz":480,"elapsed":1307658,"user":{"displayName":"Anthony Lu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTUIfTM4nze0CZKfQ2Xt4yh29AxmcfpTApeKWYiw=s64","userId":"06896095403680468971"}},"outputId":"2b18105c-84ec-423f-8d3c-d1629d51a320"},"source":["model = FastWordModel(vocab_size, num_subreddits)\r\n","LEARNING_RATE = 0.01\r\n","MOMENTUM = 0\r\n","WEIGHT_DECAY = 0\r\n","optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)    \r\n","\r\n","#try predictions\r\n","def predict(model, data):\r\n","    counter = 0\r\n","    num_correct = 0\r\n","    with torch.no_grad():\r\n","        for x,y in data:\r\n","            output = model(x)\r\n","            subreddit_number = torch.argmax(output).numpy()\r\n","            #post_title_file = open('data/post_titles.txt', 'r', encoding='utf-8')\r\n","            #post_titles = post_title_file.readlines()\r\n","            #subreddit_file = open('drive/My Drive/Colab Notebooks/subreddits.txt', 'r', encoding='utf-8') \r\n","            #subreddits = subreddit_file.readlines()\r\n","            #if subreddits[counter] == number_to_subreddit[str(subreddit_number)]:\r\n","            #    num_correct += 1\r\n","\r\n","\r\n","            if subreddit_number == y[0].numpy():\r\n","                num_correct += 1\r\n","            counter += 1\r\n","        print(num_correct)\r\n","        print(counter)\r\n","        print(str(1.0*num_correct/counter))\r\n","#predict(model, test_data)\r\n","  \r\n","def train(model, data, test, optimizer):\r\n","    num_epochs = 20\r\n","    counter = 0\r\n","    for i in range(num_epochs):\r\n","        running_loss = 0\r\n","        for x,y in data:\r\n","            optimizer.zero_grad()\r\n","            #not sure why this tensor nesting happened\r\n","            output = model(x)[0]\r\n","            #import pdb; pdb.set_trace()\r\n","            loss = model.loss(torch.unsqueeze(output, 0), y)\r\n","            #import pdb; pdb.set_trace()\r\n","            loss.backward()\r\n","            optimizer.step()\r\n","            running_loss += loss\r\n","            counter += 1\r\n","        print(\"epoch number \" + str(i) + \" has running loss \" + str(running_loss))\r\n","        predict(model, test)\r\n","        counter = 0\r\n","\r\n","train_data = formatted_data[0:int(0.8*len(formatted_data))]\r\n","test_data = formatted_data[int(0.8*len(formatted_data)):]\r\n","\r\n","train(model, train_data, test_data, optimizer)\r\n","print(\"trained\")"],"execution_count":49,"outputs":[{"output_type":"stream","text":["epoch number 0 has running loss tensor(5443.4355, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 1 has running loss tensor(4355.6094, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 2 has running loss tensor(3445.0520, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 3 has running loss tensor(2870.8315, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 4 has running loss tensor(2702.6672, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 5 has running loss tensor(2545.5081, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 6 has running loss tensor(2499.0618, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 7 has running loss tensor(2474.6760, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 8 has running loss tensor(2487.0945, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 9 has running loss tensor(2421.7690, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 10 has running loss tensor(2331.8665, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 11 has running loss tensor(2262.9360, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 12 has running loss tensor(2223.7051, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 13 has running loss tensor(2148.8616, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 14 has running loss tensor(2090.5461, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 15 has running loss tensor(2087.1072, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 16 has running loss tensor(2101.7532, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 17 has running loss tensor(2104.2815, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 18 has running loss tensor(2127.3496, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","epoch number 19 has running loss tensor(2134.3274, grad_fn=<AddBackward0>)\n","1037\n","4000\n","0.25925\n","trained\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UMSQZhkZE-sR"},"source":["\r\n","0.28025\r\n","\r\n","0.29825\r\n","\r\n","0.30225\r\n","\r\n","0.30525\r\n","\r\n","0.3085\r\n","\r\n","0.308\r\n","\r\n","0.3085\r\n","\r\n","0.30825\r\n","\r\n","0.31325\r\n","\r\n","0.3155\r\n","\r\n","0.3135\r\n","\r\n","0.31425\r\n","\r\n","0.31425\r\n","\r\n","0.31525\r\n","\r\n","0.31575\r\n","\r\n","0.31675\r\n","\r\n","0.31475\r\n","\r\n","0.317\r\n","\r\n","0.31575\r\n","\r\n","0.31475\r\n"],"execution_count":null,"outputs":[]}]}